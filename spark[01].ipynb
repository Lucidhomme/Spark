{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD는 두가지의 연산으로 이루어져 있다\n",
    "* Transformation\n",
    "* Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformation -> Lazy Execution 또는 Lazy Loading\n",
    "* 트랜스포메이션이 행해지면 RDD가 수행되는 것이 아니라 새로운 RDD를 만들어 내고 그 새로운 RDD에 수행결과를 저장하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Action\n",
    "* 메서드로 이루어진 실행작업이며, 트랜스포메이션이 행해지고 나서 이루어지는 Evaluation작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-OKGD8SP:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=sparkApp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('sparkApp')\n",
    "spark = SparkContext(conf=conf)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/test.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.textFile('./data/test.txt')\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = rdd.filter(lambda x : 'spark' in x)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD 생성\n",
    "* 데이터를 직접 만드는 방법(parallelize()), 외부 데이터를 로드 방법으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd = spark.parallelize(['test', 'this is a test rdd'])\n",
    "sample_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'this is a test rdd']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD 자주 쓰는 연산 함수\n",
    "* collect() : RDD에 트랜스포메이션된 결과를 리턴하는 함수\n",
    "* map() : 연산을 수행하고 싶을 때 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.parallelize(list(range(5)))\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = numbers.map(lambda x : x * x).collect()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* flatmap() : 중첩된 리스트들의 원소를 하나의 리스트로 flatten해서 리턴하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'spark', 'hi', 'python', 'hi', 'django', 'hi', 'sklearn']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.parallelize(['hi spark', 'hi python', 'hi django', 'hi sklearn'])\n",
    "unique_string = strings.flatMap(lambda x : x.split(' ')).collect()\n",
    "unique_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter() : 조건으로 필터링하는 연산자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = spark.parallelize(list(range(1,30,3)))\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = num.filter(lambda x : x % 2 == 0).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pair rdd란? key-value 쌍으로 이루어진 RDD\n",
    "* python tuple을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[33] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair rdd 생성\n",
    "pairRDD = spark.parallelize([(1,3),(2,4),(3,3),(1,5) ])\n",
    "pairRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 8, 2: 4, 3: 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.reduceByKey(lambda x,y : x+y).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 25, 2: 16, 3: 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.mapValues(lambda x : x**2).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x201423b1708>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x201423b1948>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x201423b1cc8>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 3, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (2, 4), (3, 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 외부 데이터를 로드해서 RDD 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/spark-rdd-name-customers.csv MapPartitionsRDD[31] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD = spark.textFile('./data/spark-rdd-name-customers.csv')\n",
    "customerRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 'Alfreds Futterkiste'),\n",
       " ('Mexico', 'Ana Trujillo Emparedados y helados'),\n",
       " ('Mexico', 'Antonio Moreno Taqueria'),\n",
       " ('UK', 'Around the Horn'),\n",
       " ('Sweden', 'Berglunds snabbkop'),\n",
       " ('Germany', 'Blauer See Delikatessen'),\n",
       " ('France', 'Blondel pere et fils'),\n",
       " ('Spain', 'Bolido Comidas preparadas'),\n",
       " ('France', \"Bon app'\"),\n",
       " ('Canada', 'Bottom-Dollar Marketse'),\n",
       " ('UK', \"B's Beverages\"),\n",
       " ('Argentina', 'Cactus Comidas para llevar'),\n",
       " ('Mexico', 'Centro comercial Moctezuma'),\n",
       " ('Switzerland', 'Chop-suey Chinese'),\n",
       " ('Brazil', 'Comercio Mineiro'),\n",
       " ('UK', 'Consolidated Holdings'),\n",
       " ('Germany', 'Drachenblut Delikatessend'),\n",
       " ('France', 'Du monde entier'),\n",
       " ('UK', 'Eastern Connection'),\n",
       " ('Austria', 'Ernst Handel'),\n",
       " ('Brazil', 'Familia Arquibaldo'),\n",
       " ('Spain', 'FISSA Fabrica Inter. Salchichas S.A.'),\n",
       " ('France', 'Folies gourmandes'),\n",
       " ('Sweden', 'Folk och fa HB'),\n",
       " ('Germany', 'Frankenversand'),\n",
       " ('France', 'France restauration'),\n",
       " ('Italy', 'Franchi S.p.A.'),\n",
       " ('Portugal', 'Furia Bacalhau e Frutos do Mar'),\n",
       " ('Spain', 'Galeria del gastronomo'),\n",
       " ('Spain', 'Godos Cocina Tipica'),\n",
       " ('Brazil', 'Gourmet Lanchonetes'),\n",
       " ('USA', 'Great Lakes Food Market'),\n",
       " ('Venezuela', 'GROSELLA-Restaurante'),\n",
       " ('Brazil', 'Hanari Carnes'),\n",
       " ('Venezuela', 'HILARIoN-Abastos'),\n",
       " ('USA', 'Hungry Coyote Import Store'),\n",
       " ('Ireland', 'Hungry Owl All-Night Grocers'),\n",
       " ('UK', 'Island Trading'),\n",
       " ('Germany', 'Koniglich Essen'),\n",
       " ('France', \"La corne d'abondance\"),\n",
       " ('France', \"La maison d'Asie\"),\n",
       " ('Canada', 'Laughing Bacchus Wine Cellars'),\n",
       " ('USA', 'Lazy K Kountry Store'),\n",
       " ('Germany', 'Lehmanns Marktstand'),\n",
       " ('USA', \"Let's Stop N Shop\"),\n",
       " ('Venezuela', 'LILA-Supermercado'),\n",
       " ('Venezuela', 'LINO-Delicateses'),\n",
       " ('USA', 'Lonesome Pine Restaurant'),\n",
       " ('Italy', 'Magazzini Alimentari Riuniti'),\n",
       " ('Belgium', 'Maison Dewey'),\n",
       " ('Canada', 'Mere Paillarde'),\n",
       " ('Germany', 'Morgenstern Gesundkost'),\n",
       " ('UK', 'North/South'),\n",
       " ('Argentina', 'Oceano Atlantico Ltda.'),\n",
       " ('USA', 'Old World Delicatessen'),\n",
       " ('Germany', 'Ottilies Kaseladen'),\n",
       " ('France', 'Paris specialites'),\n",
       " ('Mexico', 'Pericles Comidas clasicas'),\n",
       " ('Austria', 'Piccolo und mehr'),\n",
       " ('Portugal', 'Princesa Isabel Vinhoss'),\n",
       " ('Brazil', 'Que Delicia'),\n",
       " ('Brazil', 'Queen Cozinha'),\n",
       " ('Germany', 'QUICK-Stop'),\n",
       " ('Argentina', 'Rancho grande'),\n",
       " ('USA', 'Rattlesnake Canyon Grocery'),\n",
       " ('Italy', 'Reggiani Caseifici'),\n",
       " ('Brazil', 'Ricardo Adocicados'),\n",
       " ('Switzerland', 'Richter Supermarkt'),\n",
       " ('Spain', 'Romero y tomillo'),\n",
       " ('Norway', 'Sante Gourmet'),\n",
       " ('USA', 'Save-a-lot Markets'),\n",
       " ('UK', 'Seven Seas Imports'),\n",
       " ('Denmark', 'Simons bistro'),\n",
       " ('France', 'Specialites du monde'),\n",
       " ('USA', 'Split Rail Beer & Ale'),\n",
       " ('Belgium', 'Supremes delices'),\n",
       " ('USA', 'The Big Cheese'),\n",
       " ('USA', 'The Cracker Box'),\n",
       " ('Germany', 'Toms Spezialitaten'),\n",
       " ('Mexico', 'Tortuga Restaurante'),\n",
       " ('Brazil', 'Tradicao Hipermercados'),\n",
       " ('USA', \"Trail's Head Gourmet Provisioners\"),\n",
       " ('Denmark', 'Vaffeljernet'),\n",
       " ('France', 'Victuailles en stock'),\n",
       " ('France', 'Vins et alcools Chevalier'),\n",
       " ('Germany', 'Die Wandernde Kuh'),\n",
       " ('Finland', 'Wartian Herkku'),\n",
       " ('Brazil', 'Wellington Importadora'),\n",
       " ('USA', 'White Clover Markets'),\n",
       " ('Finland', 'Wilman Kala'),\n",
       " ('Poland', 'Wolski')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map연산자를 이용해서 콤마로 split하고 튜플로 리턴하는 구문을 작성해보자\n",
    "cusPairs = customerRDD.map(lambda x : (x.split(',')[1], x.split(',')[0] ))\n",
    "cusPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', <pyspark.resultiterable.ResultIterable at 0x20142373588>),\n",
       " ('Mexico', <pyspark.resultiterable.ResultIterable at 0x20142373808>),\n",
       " ('UK', <pyspark.resultiterable.ResultIterable at 0x20142373388>),\n",
       " ('Sweden', <pyspark.resultiterable.ResultIterable at 0x20142373ec8>),\n",
       " ('France', <pyspark.resultiterable.ResultIterable at 0x20142373a48>),\n",
       " ('Spain', <pyspark.resultiterable.ResultIterable at 0x20142373608>),\n",
       " ('Canada', <pyspark.resultiterable.ResultIterable at 0x20142373ac8>),\n",
       " ('Argentina', <pyspark.resultiterable.ResultIterable at 0x20142373288>),\n",
       " ('Switzerland', <pyspark.resultiterable.ResultIterable at 0x20142373c08>),\n",
       " ('Brazil', <pyspark.resultiterable.ResultIterable at 0x20142373508>),\n",
       " ('Austria', <pyspark.resultiterable.ResultIterable at 0x20142373308>),\n",
       " ('Italy', <pyspark.resultiterable.ResultIterable at 0x20142373dc8>),\n",
       " ('Portugal', <pyspark.resultiterable.ResultIterable at 0x201423643c8>),\n",
       " ('USA', <pyspark.resultiterable.ResultIterable at 0x20142364048>),\n",
       " ('Venezuela', <pyspark.resultiterable.ResultIterable at 0x20142364108>),\n",
       " ('Ireland', <pyspark.resultiterable.ResultIterable at 0x20142373c48>),\n",
       " ('Belgium', <pyspark.resultiterable.ResultIterable at 0x201423adb08>),\n",
       " ('Norway', <pyspark.resultiterable.ResultIterable at 0x201423adcc8>),\n",
       " ('Denmark', <pyspark.resultiterable.ResultIterable at 0x201423ada88>),\n",
       " ('Finland', <pyspark.resultiterable.ResultIterable at 0x201423ad748>),\n",
       " ('Poland', <pyspark.resultiterable.ResultIterable at 0x201423ad1c8>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey() : 키값을 리스트 형태로 리턴 함수\n",
    "cusPairs.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the Horn',\n",
       " \"B's Beverages\",\n",
       " 'Consolidated Holdings',\n",
       " 'Eastern Connection',\n",
       " 'Island Trading',\n",
       " 'North/South',\n",
       " 'Seven Seas Imports']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UK에 사는 고객이름만 출력한다면? (dict형식으로 만들어서)\n",
    "groupKey = cusPairs.groupByKey().collect()\n",
    "# 파이썬 문법\n",
    "# for country , names in groupKey :\n",
    "#     if country == 'UK' :\n",
    "#         for name in names :\n",
    "#             print(name)\n",
    "\n",
    "# groupKey\n",
    "\n",
    "# 스파크문법\n",
    "customerDict = {\n",
    "    country : [name for name in names] for country, names in groupKey\n",
    "    \n",
    "}\n",
    "\n",
    "customerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Argentina',\n",
       " 'Argentina',\n",
       " 'Austria',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Brazil',\n",
       " 'Brazil']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey : key 오름차순으로 정렬을 해보고 상위 10개만 뽑는다면?\n",
    "cusPairs.sortByKey().keys().collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 11),\n",
       " ('Mexico', 5),\n",
       " ('UK', 7),\n",
       " ('Sweden', 2),\n",
       " ('France', 11),\n",
       " ('Spain', 5),\n",
       " ('Canada', 3),\n",
       " ('Argentina', 3),\n",
       " ('Switzerland', 2),\n",
       " ('Brazil', 9),\n",
       " ('Austria', 2),\n",
       " ('Italy', 3),\n",
       " ('Portugal', 2),\n",
       " ('USA', 13),\n",
       " ('Venezuela', 4),\n",
       " ('Ireland', 1),\n",
       " ('Belgium', 2),\n",
       " ('Norway', 1),\n",
       " ('Denmark', 2),\n",
       " ('Finland', 2),\n",
       " ('Poland', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라별 고객이 몇명인지를 카운트 해본다면?\n",
    "# cusPairs.collect()\n",
    "mapR = cusPairs.mapValues(lambda x : 1).reduceByKey(lambda x, y : x+y)\n",
    "mapR.collect()\n",
    "# {\n",
    "#     i:j\n",
    "#     for i, j in mapR.collect()\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spark dataframe은 RDD의 확장된 구조이다\n",
    "* 행, 열로 이루어진 내장 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 생성\n",
    "* 스파크 세션을 이용한 생성\n",
    "* SQL 컨텍스트의 테이블을 통한 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x201423d7d08>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx = SQLContext(spark)\n",
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'}),\n",
       " Row(brand='Ford', models={'name': 'Focus', 'price': '18825'}),\n",
       " Row(brand='Ford', models={'name': 'Mustang', 'price': '26670'}),\n",
       " Row(brand='BMW', models={'name': '320', 'price': '40250'}),\n",
       " Row(brand='BMW', models={'name': 'X3', 'price': '41000'}),\n",
       " Row(brand='BMW', models={'name': 'X5', 'price': '60700'}),\n",
       " Row(brand='Fiat', models={'name': '500', 'price': '16495'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 파일\n",
    "# json -> RDD -> DataFrame\n",
    "sample_json = spark.textFile('./data/cars.json')\n",
    "cars_df = sqlCtx.createDataFrame(sample_json.map(lambda x : json.loads(x)) )\n",
    "cars_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- models: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|brand|              models|\n",
      "+-----+--------------------+\n",
      "| Ford|[name -> Fiesta, ...|\n",
      "| Ford|[name -> Focus, p...|\n",
      "| Ford|[name -> Mustang,...|\n",
      "|  BMW|[name -> 320, pri...|\n",
      "|  BMW|[name -> X3, pric...|\n",
      "|  BMW|[name -> X5, pric...|\n",
      "| Fiat|[name -> 500, pri...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|brand|\n",
      "+-----+\n",
      "| Ford|\n",
      "| Ford|\n",
      "| Ford|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "| Fiat|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 프레임에 대한 연산\n",
    "# select()\n",
    "\n",
    "cars_df.select('brand').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|14260|\n",
      "|18825|\n",
      "|26670|\n",
      "|40250|\n",
      "|41000|\n",
      "|60700|\n",
      "|16495|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.select('models.price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼의 타입 변환\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford| Fiesta|14260|\n",
      "| Ford|  Focus|18825|\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "| Fiat|    500|16495|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_price_type = cars_df.select('brand', 'models.name', 'models.price')\n",
    "# car_price_type\n",
    "cars_price_type = car_price_type.withColumn('price', car_price_type['price'].cast(IntegerType()))\n",
    "cars_price_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(brand='Ford', name='Fiesta', price=14260)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cars_price_type.printSchema()\n",
    "cars_price_type.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 비교연산\n",
    "# cars_price_type.collect()\n",
    "cars_price_type.filter(cars_price_type['price'] > 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|brand|count|\n",
      "+-----+-----+\n",
      "|  BMW|    3|\n",
      "| Fiat|    1|\n",
      "| Ford|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹핑\n",
    "cars_price_type.groupBy('brand').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
